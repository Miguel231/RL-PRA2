# Multi-Agent Reinforcement Learning Project

This repository contains implementations of **Independent Q-Learning (IQL)** and **Cooperative Q-Learning (CQL)** algorithms applied to multi-agent environments, including the Prisoner's Dilemma matrix game and Level-Based Foraging (LBF) tasks.

## ğŸ“‹ Project Overview

This project explores multi-agent reinforcement learning in both competitive and cooperative settings:

1. **Prisoner's Dilemma**: A classic game theory problem testing cooperation vs. defection strategies
2. **Level-Based Foraging (LBF)**: Grid-world environments requiring coordination between agents to collect food

The goal is to analyze how independent learners (IQL and CQL) perform in different reward structures and coordination requirements.

## ğŸ—ï¸ Project Structure

```
RL-PRA2/
â”œâ”€â”€ source_code/
â”‚   â”œâ”€â”€ iql.py                 # Independent Q-Learning implementation
â”‚   â”œâ”€â”€ cql.py                 # Cooperative Q-Learning implementation
â”‚   â”œâ”€â”€ train.py               # Main training script for LBF environments
â”‚   â”œâ”€â”€ train_iql.py           # Training script for Prisoner's Dilemma
â”‚   â”œâ”€â”€ video.py               # GIF generation for trained agents
â”‚   â””â”€â”€ pd_game.py             # Prisoner's Dilemma environment
â”œâ”€â”€ models/                    # Saved trained models (.pkl files)
â”œâ”€â”€ gifs/                      # Generated visualizations
â”œâ”€â”€ plots/                     # Training curves and results
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Setup

1. Clone the repository:
```bash
git clone https://github.com/Miguel231/RL-PRA2.git
cd RL-PRA2
```

2. Create a virtual environment (recommended):
```bash
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸš€ Usage

### Training Agents

#### Level-Based Foraging
Train IQL and CQL agents on both cooperative and standard foraging environments:

```bash
cd source_code
python train.py
```

This will:
- Train agents for 30,000 episodes
- Evaluate every 2,000 episodes
- Save models to `models/`
- Generate training plots in `plots/`

#### Prisoner's Dilemma
Train IQL agents on the matrix game:

```bash
python train_iql.py
```

### Generating Visualizations

Create GIFs of trained agent behaviors:

```bash
python video.py
```

Generated GIFs will be saved in `gifs/` directory, including:
- Individual agent performances
- Side-by-side comparisons of IQL vs CQL

## ğŸ“Š Environments

### Level-Based Foraging

Two variants are included:

1. **Foraging-5x5-2p-1f-v3** (Standard)
   - 5x5 grid
   - 2 players
   - 1 food item
   - Agents can collect food individually if their level is sufficient

2. **Foraging-5x5-2p-1f-coop-v3** (Cooperative)
   - Same setup as standard
   - **Requires both agents to coordinate** to collect any food
   - Tests pure cooperation

### Prisoner's Dilemma

Classic 2-player matrix game with payoff structure:

|               | Cooperate | Defect |
|---------------|-----------|--------|
| **Cooperate** | (-1, -1)  | (-5, 0)|
| **Defect**    | (0, -5)   | (-3, -3)|

## ğŸ§  Algorithms

### Independent Q-Learning (IQL)
- Each agent learns independently
- Uses Îµ-greedy exploration
- Standard Q-learning updates
- No explicit coordination mechanism

### Cooperative Q-Learning (CQL)
- Similar to IQL but designed for cooperative tasks
- Conservative Q-value updates
- Encourages coordination through reward structure
- Independent learning with cooperation incentives

### Hyperparameters

**IQL Configuration:**
- Learning rate: 0.2
- Discount factor (Î³): 0.95
- Initial Îµ: 0.9
- Evaluation Îµ: 0.05
- Episodes: 30,000

**CQL Configuration:**
- Learning rate: 0.5
- Discount factor (Î³): 0.95
- Initial Îµ: 0.9
- Evaluation Îµ: 0.05
- Episodes: 30,000

## ğŸ“ˆ Results

### Key Findings

1. **Cooperative Environment**:
   - Both IQL and CQL achieve ~85-95% success rate
   - Agents learn effective coordination
   - Stable convergence around 15,000 episodes

2. **Standard Environment**:
   - High variance and instability
   - CQL: 15-55% success (with catastrophic forgetting)
   - IQL: 25-32% success (more stable but lower performance)
   - Neither converges to stable policies

3. **Prisoner's Dilemma**:
   - IQL converges to Nash Equilibrium (Defect, Defect)
   - Returns stabilize at -3.0
   - Demonstrates classic dilemma outcome

### Interpretation

The results demonstrate that independent learners excel in cooperative settings with aligned incentives but struggle significantly in mixed-motive or competitive scenarios. The non-stationary nature of multi-agent learning prevents stable convergence when agents have conflicting objectives.

## ğŸ“ Saved Models

Trained models are saved as `.pkl` files in the `models/` directory:
- `IQL_Foraging_5x5_2p_1f_v3.pkl`
- `CQL_Foraging_5x5_2p_1f_v3.pkl`
- `IQL_Foraging_5x5_2p_1f_coop_v3.pkl`
- `CQL_Foraging_5x5_2p_1f_coop_v3.pkl`

Models can be loaded using:
```python
import dill
with open('models/IQL_Foraging_5x5_2p_1f_coop_v3.pkl', 'rb') as f:
    agent = dill.load(f)
```

## ğŸ¥ Visualizations

The `video.py` script generates:
- Individual agent GIFs showing behavior in each environment
- Comparison GIFs showing IQL vs CQL side-by-side
- Original environment sprites and graphics

## ğŸ”¬ Experimental Setup

- **Training episodes**: 30,000 per agent
- **Episode length**: 50 steps maximum
- **Evaluation frequency**: Every 2,000 episodes
- **Evaluation episodes**: 50 episodes per evaluation
- **Random seed**: 0 (for reproducibility)

## ğŸ“š Dependencies

Main libraries:
- `gymnasium`: RL environment interface
- `numpy`: Numerical computations
- `matplotlib`: Plotting and visualization
- `lbforaging`: Level-based foraging environments
- `pygame`: Rendering and visualization
- `Pillow (PIL)`: Image processing for GIFs
- `dill`: Model serialization
- `tqdm`: Progress bars

See `requirements.txt` for complete list with versions.

## ğŸ¤ Contributing

This is an academic project for UAB - Paradigms of Machine Learning course. For questions or issues, please open an issue on GitHub.

## ğŸ“„ License

This project is part of an academic assignment at Universitat AutÃ²noma de Barcelona (UAB).

## ğŸ‘¥ Authors

- Miguel231 - [GitHub Profile](https://github.com/Miguel231)

## ğŸ™ Acknowledgments

- Level-Based Foraging environment: [lb-foraging](https://github.com/semitable/lb-foraging)
- Course: Paradigms of Machine Learning, UAB
- Professor: [Course Instructor Name]

## ğŸ“§ Contact

For questions or collaboration, please reach out through GitHub issues or pull requests.

---

**Note**: This project demonstrates fundamental concepts in multi-agent reinforcement learning and is intended for educational purposes.